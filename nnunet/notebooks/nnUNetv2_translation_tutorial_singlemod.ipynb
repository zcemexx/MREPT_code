{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ------- How to : build a nnUNet_translation dataset -------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import shutil, json, glob, os\n",
    "from tqdm import tqdm \n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "data_dir = 'dummy/mr/'\n",
    "target_dir = 'dummy/ct/'\n",
    "\n",
    "os.environ['nnUNet_results'] = 'results/'\n",
    "os.environ['nnUNet_raw'] = 'raw/'\n",
    "os.environ['nnUNet_preprocessed'] = 'preprocessed/'\n",
    "\n",
    "# example with 1 input modality\n",
    "list_datas = sorted(glob.glob(os.path.join(data_dir, '*.nii.gz')))\n",
    "list_targets = sorted(glob.glob(os.path.join(target_dir, '*.nii.gz')))\n",
    "\n",
    "summ = lambda L, n=3: [os.path.basename(f) for f in (L[:n] + (['...'] if len(L) > 2*n else []) + L[-n:])]\n",
    "print(f\"Datas ({len(list_datas)}): {summ(list_datas)}\")\n",
    "print(f\"Targets ({len(list_targets)}): {summ(list_targets)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define dataset ID and make paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id =  050 # /!\\ we will use both the dataset_id and the dataset_id + 1 \n",
    "dataset_data_name = 'dummy_MR'\n",
    "dataset_target_name = 'dummy_CT'\n",
    "\n",
    "# we will copy the datas\n",
    "# do not use exist_ok=True, we want an error if the dataset exist already\n",
    "dataset_data_path = os.path.join(os.environ['nnUNet_raw'], f'Dataset{dataset_id:03d}_{dataset_data_name}') \n",
    "os.makedirs(dataset_data_path, exist_ok = True)\n",
    "os.makedirs(os.path.join(dataset_data_path, 'imagesTr'), exist_ok=True)\n",
    "os.makedirs(os.path.join(dataset_data_path, 'labelsTr'), exist_ok = True)\n",
    "\n",
    "dataset_target_path = os.path.join(os.environ['nnUNet_raw'], f'Dataset{dataset_id+1:03d}_{dataset_target_name}') \n",
    "os.makedirs(dataset_target_path, exist_ok = True)\n",
    "os.makedirs(os.path.join(dataset_target_path, 'imagesTr'), exist_ok = True)\n",
    "os.makedirs(os.path.join(dataset_target_path, 'labelsTr'), exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy files and create dummy masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(data_path, dataset_path, mat):\n",
    "    curr_nifti = nib.load(data_path)\n",
    "    filename = os.path.basename(data_path)\n",
    "    if not filename.endswith('_0000.nii.gz'):\n",
    "        filename = filename.replace('.nii.gz', '_0000.nii.gz')\n",
    "    curr_nifti.to_filename(os.path.join(dataset_path, f'imagesTr/{filename}'))\n",
    "\n",
    "    data = curr_nifti.get_fdata()\n",
    "    # Adjust the mask as needed for your specific use case. By default, the mask is set to 1 for the entire volume.\n",
    "    # This will be used for foreground preprocessing, cf https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/explanation_normalization.md\n",
    "    data = np.ones_like(data)\n",
    "\n",
    "    filename = filename.replace('_0000', '') #remove _0000 for masks\n",
    "    nib.Nifti1Image(data, mat).to_filename(os.path.join(dataset_path, f'labelsTr/{filename}'))\n",
    "\n",
    "mat = nib.load(list_datas[-1]).affine\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    list(tqdm(executor.map(lambda data_path: process_file(data_path, dataset_data_path, mat), list_datas), total=len(list_datas)))\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    list(tqdm(executor.map(lambda target_path: process_file(target_path, dataset_target_path, mat), list_targets), total=len(list_targets)))\n",
    "\n",
    "#### without multithreading\n",
    "# for data_path in tqdm(list_datas, total=len(list_datas)):\n",
    "#     process_file(data_path, dataset_data_path, mat)\n",
    "\n",
    "# for target_path in tqdm(list_targets, total=len(list_targets)):\n",
    "#     process_file(target_path, dataset_target_path, mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the dataset.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /!\\ you will need to edit this with regards to the number of modalities used;\n",
    "data_dataset_json = {\n",
    "    \"labels\": {\n",
    "        \"label_001\": \"1\", \n",
    "        \"background\": 0\n",
    "    },\n",
    "    \"channel_names\": {\n",
    "        \"0\": \"MR\",\n",
    "    },\n",
    "    \"numTraining\": len(list_datas),\n",
    "    \"file_ending\": \".nii.gz\"\n",
    "}\n",
    "dump_data_datasets_path = os.path.join(dataset_data_path, 'dataset.json')\n",
    "with open(dump_data_datasets_path, 'w') as f:\n",
    "    json.dump(data_dataset_json, f)\n",
    "\n",
    "target_dataset_json = {\n",
    "    \"labels\": {\n",
    "        \"label_001\": \"1\",\n",
    "        \"background\": 0\n",
    "    },\n",
    "    \"channel_names\": {\n",
    "        \"0\": \"CTNormalization\",\n",
    "    },\n",
    "    \"numTraining\": len(list_targets),\n",
    "    \"file_ending\": \".nii.gz\"\n",
    "}\n",
    "dump_target_datasets_path = os.path.join(dataset_target_path, 'dataset.json')\n",
    "with open(dump_target_datasets_path, 'w') as f:\n",
    "    json.dump(target_dataset_json, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply preprocessing and unpacking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 'MPLBACKEND' in os.environ: \n",
    "    del os.environ['MPLBACKEND'] # avoid conflicts with matplotlib backend  \n",
    "    \n",
    "os.system(f'nnUNetv2_plan_and_preprocess -d {dataset_id} -c 3d_fullres')\n",
    "os.system(f'nnUNetv2_unpack {dataset_id} 3d_fullres 0')\n",
    "\n",
    "os.system(f'nnUNetv2_plan_and_preprocess -d {dataset_id + 1} -c 3d_fullres')\n",
    "os.system(f'nnUNetv2_unpack {dataset_id + 1} 3d_fullres 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update 10/2025: new baseline! Using ResidualUNet instead of UNet trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(f'nnUNetv2_plan_experiment -d 101 -c 3d_fullres -pl nnUNetPlannerResUNet') #with trilinear decoder (recommended)\n",
    "#os.system(f'nnUNetv2_plan_experiment -d 101 -c 3d_fullres -pl nnUNetPlannerResUNet_standard') #with standard decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define 2nd modality raw data as gt_segmentations of 1st modality\n",
    "##### originally used for computing metrics / postprocessing, not sure if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nnunet_datas_preprocessed_dir = os.path.join(os.environ['nnUNet_preprocessed'], f'Dataset{dataset_id+1:03d}_{dataset_target_name}') \n",
    "nnunet_targets_preprocessed_dir = os.path.join(os.environ['nnUNet_preprocessed'], f'Dataset{dataset_id:03d}_{dataset_data_name}') \n",
    "\n",
    "list_targets = glob.glob(os.path.join(f\"{dataset_target_path}/imagesTr\", '*'))\n",
    "list_targets.sort()\n",
    "list_gt_segmentations_datas = glob.glob(os.path.join(f\"{nnunet_targets_preprocessed_dir}/gt_segmentations\", '*'))\n",
    "list_gt_segmentations_datas.sort()\n",
    "\n",
    "print(nnunet_targets_preprocessed_dir)\n",
    "\n",
    "for (preprocessed_path, gt_path) in zip(list_targets, list_gt_segmentations_datas):\n",
    "    # here, gt_path is the path to the gt_segmentation in nnUNet_preprocessed.\n",
    "    print(preprocessed_path, \"->\", gt_path) # ensure correct file pairing; \n",
    "    shutil.copy(src = preprocessed_path, dst = gt_path) # we use shutil.copy to ensure safety, but switching to shutil.move would be more efficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define 2nd modality preprocessed files as ground truth of 1st modality\n",
    "##### used in training, definitely needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_preprocessed_datas_seg_path = sorted(glob.glob(os.path.join(nnunet_targets_preprocessed_dir, 'nnUNetPlans_3d_fullres/*_seg.npy')))\n",
    "list_preprocessed_targets_path = sorted(glob.glob(os.path.join(nnunet_datas_preprocessed_dir, 'nnUNetPlans_3d_fullres/*.npy')))\n",
    "list_preprocessed_targets_path = [name for name in list_preprocessed_targets_path if '_seg' not in name]\n",
    "\n",
    "for (datas_path, targets_path) in zip(list_preprocessed_datas_seg_path, list_preprocessed_targets_path):\n",
    "    print(targets_path, \"->\", datas_path)\n",
    "    shutil.copy(src = targets_path, dst = datas_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### That's it!\n",
    "You should be able to start training with : \n",
    "```\n",
    "export nnUNet_raw=\"/data/alonguefosse/nnUNet/raw\"\n",
    "export nnUNet_preprocessed=\"/data/alonguefosse/nnUNet/preprocessed\"\n",
    "export nnUNet_results=\"/data/alonguefosse/nnUNet/results\"\n",
    "\n",
    "nnUNetv2_train 50 3d_fullres 0 -tr nnUNetTrainerMRCT -pl nnResUNetPlans\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
